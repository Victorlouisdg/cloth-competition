{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO dataset creation\n",
    "\n",
    "In this notebook we show how you can process the competition images into a [COCO Keypoint dataset](https://cocodataset.org/#format-data).\n",
    "\n",
    "We will convert the depth maps to images to train our keypoint detector on. However note that the color images can also be used for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from cloth_tools.dataset.format import load_competition_observation\n",
    "from airo_camera_toolkit.image_transforms.transforms.crop import Crop\n",
    "from cloth_tools.annotation.grasp_annotation import GraspAnnotation\n",
    "\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "dataset_dir = data_dir / \"cloth_competition_dataset_0000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdirs = [ f.path for f in os.scandir(dataset_dir) if f.is_dir() ]\n",
    "sample_dirs = [d for d in subdirs if \"sample_\" in d]\n",
    "sample_dirs = sorted(sample_dirs)\n",
    "sample_dirs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "depth_maps = []\n",
    "clicked_points = []\n",
    "\n",
    "for sample_dir in sample_dirs:\n",
    "    observation_start_dir = Path(sample_dir) / \"observation_start\"\n",
    "    observation = load_competition_observation(observation_start_dir)\n",
    "    images.append(observation.image_left)\n",
    "    depth_maps.append(observation.depth_map)\n",
    "\n",
    "    grasp_dir = Path(sample_dir) / \"grasp\"\n",
    "    grasp_annotation_file = grasp_dir / \"grasp_annotation.json\"\n",
    "\n",
    "    with open(grasp_annotation_file, \"r\") as f:\n",
    "        grasp_annotation = GraspAnnotation.model_validate_json(f.read())\n",
    "\n",
    "    clicked_points.append(grasp_annotation.clicked_point_frontal) # .copy())\n",
    "\n",
    "\n",
    "N_VISUALIZE = 5\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i, image in enumerate(images[:N_VISUALIZE]):\n",
    "    plt.subplot(1, N_VISUALIZE, i + 1)\n",
    "    plt.imshow(image)\n",
    "\n",
    "print(clicked_points[:N_VISUALIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_middle = observation.depth_map.shape[1] // 2\n",
    "crop_width = 350\n",
    "\n",
    "x = x_middle - crop_width // 2\n",
    "y = 240\n",
    "crop_height = 600\n",
    "\n",
    "crop_rgb_left = Crop(observation.image_left.shape, x=x, y=y, w=crop_width, h=crop_height)\n",
    "\n",
    "images_cropped = [crop_rgb_left.transform_image(image) for image in images]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i, (image, clicked_point) in enumerate(zip(images_cropped[:N_VISUALIZE], clicked_points[:N_VISUALIZE])):\n",
    "    plt.subplot(1, N_VISUALIZE, i + 1)\n",
    "    \n",
    "    point_in_crop = crop_rgb_left.transform_point(clicked_point)\n",
    "    plt.scatter(*point_in_crop, c=\"lawngreen\", s=100, marker=\"x\")\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_max = 1.55\n",
    "segmentation_masks = [d < distance_max for d in depth_maps]\n",
    "segmentation_masks_cropped = [crop_rgb_left.transform_image(mask) for mask in segmentation_masks]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i, mask in enumerate(segmentation_masks_cropped[:N_VISUALIZE]):\n",
    "    plt.subplot(1, N_VISUALIZE, i + 1)\n",
    "    plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_maps_cropped = [crop_rgb_left.transform_image(d) for d in depth_maps]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i, d in enumerate(depth_maps_cropped[:N_VISUALIZE]):\n",
    "    plt.subplot(1, N_VISUALIZE, i + 1)\n",
    "    plt.imshow(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_maps_scaled = []\n",
    "\n",
    "for depth_cropped, segmentation_cropped in zip(depth_maps_cropped, segmentation_masks_cropped):\n",
    "\n",
    "    depth_cropped_segmented = depth_cropped * segmentation_cropped\n",
    "\n",
    "    distance_cloth_min = np.min(depth_cropped)\n",
    "    distance_cloth_max = np.max(depth_cropped_segmented)\n",
    "\n",
    "    distance_cloth_range = distance_cloth_max - distance_cloth_min\n",
    "\n",
    "    print(f\"Cloth distance range: {distance_cloth_min:.2f}-{distance_cloth_max:.2f} m ({distance_cloth_range:.2f} m)\")\n",
    "\n",
    "    depth_scaled = (depth_cropped_segmented - distance_cloth_min) / distance_cloth_range\n",
    "\n",
    "    depth_scaled[segmentation_cropped == False] = 1.0 # set background to max value\n",
    "\n",
    "    depth_maps_scaled.append(depth_scaled)\n",
    "\n",
    "print(f\"Scaled depth map value range: {np.min(depth_maps_scaled[0])}-{np.max(depth_maps_scaled[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for i, (depth_map, clicked_point) in enumerate(zip(depth_maps_scaled[:N_VISUALIZE], clicked_points[:N_VISUALIZE])):\n",
    "    plt.subplot(1, N_VISUALIZE, i + 1)\n",
    "    plt.imshow(depth_map)\n",
    "\n",
    "    point_in_crop = crop_rgb_left.transform_point(clicked_point)\n",
    "    plt.scatter(*point_in_crop, c='r', s=300, marker=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airo_dataset_tools.data_parsers.coco import (\n",
    "    CocoInfo,\n",
    "    CocoKeypointCategory,\n",
    "    CocoLicense,\n",
    "    CocoImage,\n",
    "    CocoKeypointAnnotation,\n",
    "    CocoKeypointsDataset,\n",
    ")\n",
    "\n",
    "\n",
    "coco_info = CocoInfo(\n",
    "    description=\"Preprocessed depth maps from the ICRA 2024 Cloth Competition dataset.\",\n",
    "    url=\"https://github.com/Victorlouisdg/cloth-competition\",\n",
    "    version=\"0.1\",\n",
    "    year=2024,\n",
    "    contributor=\"Victor-Louis De Gusseme & Thomas Lips\",\n",
    "    date_created=\"2024/02/22\",\n",
    ")\n",
    "\n",
    "coco_license = CocoLicense(\n",
    "    id=1,\n",
    "    name=\"Attribution-NonCommercial-ShareAlike License\",  # TODO change to a more suitable license\n",
    "    url=\"http://creativecommons.org/licenses/by-nc-sa/2.0/\",\n",
    ")\n",
    "\n",
    "coco_keypoint_category = CocoKeypointCategory(\n",
    "    supercategory=\"cloth\",\n",
    "    id=0,\n",
    "    name=\"tshirt\",\n",
    "    keypoints=[\"grasp_annotated\"],\n",
    ")\n",
    "\n",
    "print(repr(coco_info))\n",
    "print(repr(coco_license))\n",
    "print(repr(coco_keypoint_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory to store the preprocessed images\n",
    "import datetime\n",
    "\n",
    "datetime_str = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "preprocessed_dir = data_dir / f\"dataset_preprocessed_{datetime_str}\"\n",
    "images_preprocessed_dir = preprocessed_dir / \"images\"\n",
    "images_preprocessed_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image_paths = []\n",
    "\n",
    "for i, depth_map_scaled in enumerate(depth_maps_scaled):\n",
    "    image_name = f\"image_{i:06}.png\"\n",
    "    image_path = str(images_preprocessed_dir / image_name)\n",
    "    image_paths.append(image_path)\n",
    "    cv2.imwrite(image_path, depth_map_scaled * 255.0)\n",
    "\n",
    "\n",
    "image_paths_relative = [os.path.relpath(path, preprocessed_dir) for path in image_paths]\n",
    "\n",
    "coco_images = []\n",
    "\n",
    "for i, path in enumerate(image_paths_relative):\n",
    "    coco_image = CocoImage(\n",
    "        id=1,\n",
    "        width=crop_width,\n",
    "        height=crop_height,\n",
    "        file_name=path,\n",
    "        license=1,\n",
    "        date_captured=\"2024/02/22\",\n",
    "    )\n",
    "    coco_images.append(coco_image)\n",
    "\n",
    "print(repr(coco_images[0]))\n",
    "print(repr(coco_images[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airo_dataset_tools.segmentation_mask_converter import BinarySegmentationMask\n",
    "\n",
    "\n",
    "coco_annotations = []\n",
    "\n",
    "for i, (clicked_point, segmentation_mask) in enumerate(zip(clicked_points, segmentation_masks_cropped)):\n",
    "    x, y = clicked_point\n",
    "    v = 2 # means labelled and visible\n",
    "\n",
    "    mask = BinarySegmentationMask(segmentation_mask)\n",
    "    coco_annotation = CocoKeypointAnnotation(\n",
    "        id=i,\n",
    "        image_id=i, # Note: image_id == id only when exactly one annotation per image\n",
    "        category_id=coco_keypoint_category.id,\n",
    "        segmentation=mask.as_compressed_rle,\n",
    "        area=mask.area,\n",
    "        bbox=mask.bbox,\n",
    "        iscrowd=0,\n",
    "        keypoints=[x, y, v],\n",
    "        num_keypoints=1,\n",
    "    )\n",
    "    coco_annotations.append(coco_annotation)\n",
    "\n",
    "print(repr(coco_annotations[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_keypoints = CocoKeypointsDataset(\n",
    "    info=coco_info,\n",
    "    licenses=[coco_license],\n",
    "    images=coco_images,\n",
    "    categories=[coco_keypoint_category],\n",
    "    annotations=coco_annotations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "annotations_file = preprocessed_dir / \"annotations.json\"\n",
    "\n",
    "with open(annotations_file, \"w\") as file:\n",
    "    json.dump(coco_keypoints.model_dump(exclude_none=True), file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloth-competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
