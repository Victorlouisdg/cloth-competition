{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grasp annotation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "from airo_camera_toolkit.point_clouds.conversions import open3d_to_point_cloud, point_cloud_to_open3d\n",
    "from airo_camera_toolkit.point_clouds.operations import filter_point_cloud\n",
    "from cloth_tools.dataset.format import load_competition_observation\n",
    "import matplotlib.pyplot as plt\n",
    "from airo_camera_toolkit.utils.image_converter import ImageConverter\n",
    "import open3d as o3d\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "dataset_dir = data_dir / \"dataset_dev_0000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.dataset.format import load_competition_observation\n",
    "\n",
    "\n",
    "observation_start_dir = dataset_dir / \"sample_000000\" / \"start_observation\"\n",
    "\n",
    "observation = load_competition_observation(observation_start_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(observation.image_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_map = observation.confidence_map\n",
    "point_cloud = observation.point_cloud\n",
    "\n",
    "# Transform point cloud to world frame\n",
    "# X_W_C = observation.camera_pose_in_world  # X_LCB_C (camera pose in the left-arm base frame)\n",
    "# pcd_in_camera = point_cloud_to_open3d(point_cloud_in_camera)  # X_C_PC, need X_W_C\n",
    "# pcd = pcd_in_camera.transform(X_W_C)  # transform to world frame\n",
    "# point_cloud = open3d_to_point_cloud(pcd)\n",
    "\n",
    "# Filter outs point with low depth confidence (i.e. with high value in the confidence map)\n",
    "confidence_threshold = 1.0\n",
    "confidence_mask = (confidence_map <= confidence_threshold).reshape(-1)  # Threshold and flatten\n",
    "point_cloud_filtered = filter_point_cloud(point_cloud, confidence_mask)\n",
    "pcd_filtered = point_cloud_to_open3d(point_cloud_filtered)\n",
    "pcd_filtered.point.positions.dtype, pcd_filtered.point.colors.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.annotation.grasp_annotation import top_down_camera_pose\n",
    "\n",
    "virtual_camera_pose = top_down_camera_pose(height=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.visualization.open3d import open3d_camera\n",
    "\n",
    "color_frontal_rgb = (1, 1, 0)\n",
    "color_topdown_rgb = (0, 1, 1)\n",
    "\n",
    "world_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n",
    "\n",
    "# Visualize the cameras\n",
    "resolution = observation.camera_resolution\n",
    "intrinsics = observation.camera_intrinsics\n",
    "\n",
    "X_W_VC = virtual_camera_pose\n",
    "X_W_C = observation.camera_pose_in_world\n",
    "camera_frontal_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n",
    "camera_frontal_frame.transform(X_W_C)\n",
    "camera_top_down_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n",
    "camera_top_down_frame.transform(X_W_VC)\n",
    "camera_frontal_lines = open3d_camera(X_W_C, intrinsics, resolution, color_frontal_rgb, scale=0.2)\n",
    "camera_topdown_lines = open3d_camera(X_W_VC, intrinsics, resolution, color_topdown_rgb, scale=0.2)\n",
    "\n",
    "o3d.visualization.draw_geometries(\n",
    "    [\n",
    "        pcd_filtered.to_legacy(),\n",
    "        world_frame,\n",
    "        camera_frontal_frame,\n",
    "        camera_top_down_frame,\n",
    "        camera_frontal_lines,\n",
    "        camera_topdown_lines,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.annotation.grasp_annotation import project_point_cloud_to_image\n",
    "\n",
    "image_topdown = project_point_cloud_to_image(\n",
    "    point_cloud_filtered, X_W_VC, intrinsics, resolution, background_color=(90, 90, 90)\n",
    ")\n",
    "\n",
    "# Matplotlib seems to do some anti-aliasing, which makes the image look better than in opencv (without blurring)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(ImageConverter.from_opencv_format(image_topdown).image_in_numpy_int_format)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some experiments with blurring to make the image look better in opencv\n",
    "width = 800\n",
    "height = 400\n",
    "background_color = (120, 120, 120)\n",
    "\n",
    "image_topdown = project_point_cloud_to_image(point_cloud_filtered, X_W_VC, intrinsics, resolution, background_color)\n",
    "\n",
    "window_name_original = \"Original\"\n",
    "cv2.namedWindow(window_name_original, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name_original, width, height)\n",
    "cv2.moveWindow(window_name_original, 0, 0)\n",
    "cv2.imshow(window_name_original, image_topdown)\n",
    "\n",
    "window_name_median_blur = \"Median Blur\"\n",
    "cv2.namedWindow(window_name_median_blur, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name_median_blur, width, height)\n",
    "cv2.moveWindow(window_name_median_blur, width, 0)\n",
    "image_median_blur = cv2.medianBlur(image_topdown, 3)\n",
    "cv2.imshow(window_name_median_blur, image_median_blur)\n",
    "\n",
    "window_name_blur = \"Blur\"\n",
    "cv2.namedWindow(window_name_blur, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name_blur, width, height)\n",
    "cv2.moveWindow(window_name_blur, 0, height)\n",
    "image_blur = cv2.blur(image_topdown, (3, 3))\n",
    "cv2.imshow(window_name_blur, image_blur)\n",
    "\n",
    "window_name_combined = \"Blur + Median\"\n",
    "cv2.namedWindow(window_name_combined, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name_combined, width, height)\n",
    "cv2.moveWindow(window_name_combined, width, height)\n",
    "image_blur_median = cv2.medianBlur(image_blur, 3)\n",
    "cv2.imshow(window_name_combined, image_blur_median)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.annotation.grasp_annotation import get_manual_grasp_annotation\n",
    "\n",
    "grasp_info = get_manual_grasp_annotation(\n",
    "    observation.image_left, observation.depth_map, point_cloud_filtered, X_W_C, intrinsics, log_to_rerun=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_info.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_info.grasp_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from pydantic import BaseModel\n",
    "from cloth_tools.annotation.grasp_annotation import GraspAnnotationInfo\n",
    "from airo_dataset_tools.data_parsers.pose import Pose\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# class Keypoint2D(BaseModel):\n",
    "#     x: float\n",
    "#     y: float\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_tuple(cls, point: Tuple[int, int]):\n",
    "#         return cls(x=point[0], y=point[1])\n",
    "\n",
    "# clicked_point_frontal=Keypoint2D.from_tuple(grasp_info.clicked_point_frontal),\n",
    "# clicked_point_topdown=Keypoint2D.from_tuple(grasp_info.clicked_point_topdown),\n",
    "\n",
    "class GraspAnnotation(BaseModel):\n",
    "    clicked_point_frontal: Tuple[int, int]\n",
    "    clicked_point_topdown: Tuple[int, int]\n",
    "    grasp_depth: float\n",
    "\n",
    "def save_grasp_info(dir: str, grasp_info: GraspAnnotationInfo):\n",
    "\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "    grasp_pose_file = os.path.join(dir, \"grasp_pose.json\")\n",
    "\n",
    "    with open(grasp_pose_file, \"w\") as f:\n",
    "        grasp_pose_model = Pose.from_homogeneous_matrix(grasp_info.grasp_pose)\n",
    "        json.dump(grasp_pose_model.model_dump(exclude_none=True), f, indent=4)\n",
    "\n",
    "    grasp_annotation_file = os.path.join(dir, \"grasp_annotation.json\")\n",
    "    grasp_annotation = GraspAnnotation(\n",
    "        clicked_point_frontal=grasp_info.clicked_point_frontal,\n",
    "        clicked_point_topdown=grasp_info.clicked_point_topdown,\n",
    "        grasp_depth=grasp_info.grasp_depth,\n",
    "    )\n",
    "\n",
    "    with open(grasp_annotation_file, \"w\") as f:\n",
    "        json.dump(grasp_annotation.model_dump(exclude_none=True), f, indent=4)\n",
    "\n",
    "    # Save the two images with the grasp visualized\n",
    "    grasp_frontal_image_file = os.path.join(dir, \"frontal_image_grasp.jpg\")\n",
    "    grasp_topdown_image_file = os.path.join(dir, \"topdown_image_grasp.jpg\")\n",
    "    cv2.imwrite(grasp_frontal_image_file, grasp_info.image_frontal)\n",
    "    cv2.imwrite(grasp_topdown_image_file, grasp_info.image_topdown)\n",
    "\n",
    "\n",
    "grasp_dir = \"grasp\"\n",
    "save_grasp_info(grasp_dir, grasp_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO FIX\n",
    "def load_grasp_info(dir: str) -> GraspAnnotationInfo:\n",
    "    grasp_pose_file = os.path.join(dir, \"grasp_pose.json\")\n",
    "    with open(grasp_pose_file, \"r\") as f:\n",
    "        grasp_pose_model = Pose(**json.load(f))\n",
    "\n",
    "    grasp_annotation_file = os.path.join(dir, \"grasp_annotation.json\")\n",
    "    with open(grasp_annotation_file, \"r\") as f:\n",
    "        grasp_annotation = GraspAnnotation(**json.load(f))\n",
    "\n",
    "    grasp_frontal_image_file = os.path.join(dir, \"frontal_image_grasp.jpg\")\n",
    "    grasp_topdown_image_file = os.path.join(dir, \"topdown_image_grasp.jpg\")\n",
    "    image_frontal = cv2.imread(grasp_frontal_image_file)\n",
    "    image_topdown = cv2.imread(grasp_topdown_image_file)\n",
    "\n",
    "    return GraspAnnotationInfo(\n",
    "        grasp_pose=grasp_pose_model.homogeneous_matrix,\n",
    "        clicked_point_frontal=grasp_annotation.clicked_point_frontal,\n",
    "        clicked_point_topdown=grasp_annotation.clicked_point_topdown,\n",
    "        grasp_depth=grasp_annotation.grasp_depth,\n",
    "        image_frontal=image_frontal,\n",
    "        image_topdown=image_topdown,\n",
    "    )\n",
    "\n",
    "grasp_info_loaded = load_grasp_info(grasp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloth-competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
