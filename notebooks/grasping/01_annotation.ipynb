{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grasp annotation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "from airo_camera_toolkit.point_clouds.conversions import point_cloud_to_open3d\n",
    "from airo_camera_toolkit.point_clouds.operations import filter_point_cloud\n",
    "from cloth_tools.dataset.format import load_competition_observation\n",
    "import matplotlib.pyplot as plt\n",
    "from airo_camera_toolkit.utils.image_converter import ImageConverter\n",
    "import open3d as o3d\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "dataset_dir = data_dir / \"cloth_competition_dataset_0000\"\n",
    "sample_dir = dataset_dir / \"sample_000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_start_dir = sample_dir / \"observation_start\"\n",
    "\n",
    "observation = load_competition_observation(observation_start_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(observation.image_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_map = observation.confidence_map\n",
    "point_cloud = observation.point_cloud\n",
    "\n",
    "# Filter outs point with low depth confidence (i.e. with high value in the confidence map)\n",
    "confidence_threshold = 1.0\n",
    "confidence_mask = (confidence_map <= confidence_threshold).reshape(-1)  # Threshold and flatten\n",
    "point_cloud_filtered = filter_point_cloud(point_cloud, confidence_mask)\n",
    "pcd_filtered = point_cloud_to_open3d(point_cloud_filtered)\n",
    "pcd_filtered.point.positions.dtype, pcd_filtered.point.colors.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.annotation.grasp_annotation import top_down_camera_pose\n",
    "\n",
    "virtual_camera_pose = top_down_camera_pose(height=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell below will show the filtered point cloud in open3d. Press 'q' to close the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.visualization.open3d import open3d_camera\n",
    "\n",
    "color_frontal_rgb = (1, 1, 0)\n",
    "color_topdown_rgb = (0, 1, 1)\n",
    "\n",
    "world_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n",
    "\n",
    "# Visualize the cameras\n",
    "resolution = observation.camera_resolution\n",
    "intrinsics = observation.camera_intrinsics\n",
    "\n",
    "X_W_VC = virtual_camera_pose\n",
    "X_W_C = observation.camera_pose_in_world\n",
    "camera_frontal_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n",
    "camera_frontal_frame.transform(X_W_C)\n",
    "camera_top_down_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n",
    "camera_top_down_frame.transform(X_W_VC)\n",
    "camera_frontal_lines = open3d_camera(X_W_C, intrinsics, resolution, color_frontal_rgb, scale=0.2)\n",
    "camera_topdown_lines = open3d_camera(X_W_VC, intrinsics, resolution, color_topdown_rgb, scale=0.2)\n",
    "\n",
    "o3d.visualization.draw_geometries(\n",
    "    [\n",
    "        pcd_filtered.to_legacy(),\n",
    "        world_frame,\n",
    "        camera_frontal_frame,\n",
    "        camera_top_down_frame,\n",
    "        camera_frontal_lines,\n",
    "        camera_topdown_lines,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.annotation.grasp_annotation import project_point_cloud_to_image\n",
    "\n",
    "image_topdown = project_point_cloud_to_image(\n",
    "    point_cloud_filtered, X_W_VC, intrinsics, resolution, background_color=(90, 90, 90)\n",
    ")\n",
    "\n",
    "# Matplotlib seems to do some anti-aliasing, which makes the image look better than in opencv (without blurring)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(ImageConverter.from_opencv_format(image_topdown).image_in_numpy_int_format)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.annotation.grasp_annotation import get_manual_grasp_annotation\n",
    "\n",
    "grasp_info = get_manual_grasp_annotation(\n",
    "    observation.image_left, observation.depth_map, point_cloud_filtered, X_W_C, intrinsics, log_to_rerun=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_info.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_info.grasp_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.annotation.grasp_annotation import GraspAnnotationInfo, save_grasp_info\n",
    "from airo_dataset_tools.data_parsers.pose import Pose\n",
    "import json\n",
    "import os\n",
    "\n",
    "grasp_dir = Path(\"grasp_new\")\n",
    "save_grasp_info(grasp_dir, grasp_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.annotation.grasp_annotation import GraspAnnotation\n",
    "\n",
    "grasp_annotation_file = grasp_dir / \"grasp_annotation.json\"\n",
    "\n",
    "with open(grasp_annotation_file, \"r\") as f:\n",
    "    grasp_annotation = GraspAnnotation.model_validate_json(f.read())\n",
    "\n",
    "grasp_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf grasp_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloth-competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
