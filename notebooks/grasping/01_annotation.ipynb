{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grasp annotation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "from airo_camera_toolkit.point_clouds.conversions import open3d_to_point_cloud, point_cloud_to_open3d\n",
    "from airo_camera_toolkit.point_clouds.operations import filter_point_cloud\n",
    "from cloth_tools.dataset.format import load_competition_input_sample\n",
    "import matplotlib.pyplot as plt\n",
    "from cloth_tools.dataset.format import load_competition_input_sample\n",
    "from airo_camera_toolkit.utils.image_converter import ImageConverter\n",
    "import open3d as o3d\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "dataset_dir = data_dir / \"dataset_0000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = load_competition_input_sample(dataset_dir, sample_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample.image_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_map = sample.confidence_map\n",
    "point_cloud_in_camera = sample.point_cloud\n",
    "\n",
    "# Transform point cloud to world frame\n",
    "X_W_C = sample.camera_pose_in_world  # X_LCB_C (camera pose in the left-arm base frame)\n",
    "pcd_in_camera = point_cloud_to_open3d(point_cloud_in_camera)  # X_C_PC, need X_W_C\n",
    "pcd = pcd_in_camera.transform(X_W_C)  # transform to world frame\n",
    "point_cloud = open3d_to_point_cloud(pcd)\n",
    "\n",
    "# Filter outs point with low depth confidence (i.e. with high value in the confidence map)\n",
    "confidence_threshold = 1.0\n",
    "confidence_mask = (confidence_map <= confidence_threshold).reshape(-1)  # Threshold and flatten\n",
    "point_cloud_filtered = filter_point_cloud(point_cloud, confidence_mask)\n",
    "pcd_filtered = point_cloud_to_open3d(point_cloud_filtered)\n",
    "pcd_filtered.point.positions.dtype, pcd_filtered.point.colors.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.annotation.grasp_annotation import top_down_camera_pose\n",
    "\n",
    "virtual_camera_pose = top_down_camera_pose(height=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.visualization.open3d import open3d_camera\n",
    "\n",
    "color_frontal_rgb = (1, 1, 0)\n",
    "color_topdown_rgb = (0, 1, 1)\n",
    "\n",
    "world_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n",
    "\n",
    "# Visualize the cameras\n",
    "resolution = sample.camera_resolution\n",
    "intrinsics = sample.camera_intrinsics\n",
    "\n",
    "X_W_VC = virtual_camera_pose\n",
    "camera_frontal_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n",
    "camera_frontal_frame.transform(X_W_C)\n",
    "camera_top_down_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n",
    "camera_top_down_frame.transform(X_W_VC)\n",
    "camera_frontal_lines = open3d_camera(X_W_C, intrinsics, resolution, color_frontal_rgb, scale=0.2)\n",
    "camera_topdown_lines = open3d_camera(X_W_VC, intrinsics, resolution, color_topdown_rgb, scale=0.2)\n",
    "\n",
    "o3d.visualization.draw_geometries(\n",
    "    [\n",
    "        pcd_filtered.to_legacy(),\n",
    "        world_frame,\n",
    "        camera_frontal_frame,\n",
    "        camera_top_down_frame,\n",
    "        camera_frontal_lines,\n",
    "        camera_topdown_lines,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.annotation.grasp_annotation import project_point_cloud_to_image\n",
    "\n",
    "image_topdown = project_point_cloud_to_image(\n",
    "    point_cloud_filtered, X_W_VC, intrinsics, resolution, background_color=(90, 90, 90)\n",
    ")\n",
    "\n",
    "# Matplotlib seems to do some anti-aliasing, which makes the image look better than in opencv (without blurring)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(ImageConverter.from_opencv_format(image_topdown).image_in_numpy_int_format)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some experiments with blurring to make the image look better in opencv\n",
    "width = 800\n",
    "height = 400\n",
    "background_color = (120, 120, 120)\n",
    "\n",
    "image_topdown = project_point_cloud_to_image(point_cloud_filtered, X_W_VC, intrinsics, resolution, background_color)\n",
    "\n",
    "window_name_original = \"Original\"\n",
    "cv2.namedWindow(window_name_original, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name_original, width, height)\n",
    "cv2.moveWindow(window_name_original, 0, 0)\n",
    "cv2.imshow(window_name_original, image_topdown)\n",
    "\n",
    "window_name_median_blur = \"Median Blur\"\n",
    "cv2.namedWindow(window_name_median_blur, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name_median_blur, width, height)\n",
    "cv2.moveWindow(window_name_median_blur, width, 0)\n",
    "image_median_blur = cv2.medianBlur(image_topdown, 3)\n",
    "cv2.imshow(window_name_median_blur, image_median_blur)\n",
    "\n",
    "window_name_blur = \"Blur\"\n",
    "cv2.namedWindow(window_name_blur, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name_blur, width, height)\n",
    "cv2.moveWindow(window_name_blur, 0, height)\n",
    "image_blur = cv2.blur(image_topdown, (3, 3))\n",
    "cv2.imshow(window_name_blur, image_blur)\n",
    "\n",
    "window_name_combined = \"Blur + Median\"\n",
    "cv2.namedWindow(window_name_combined, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name_combined, width, height)\n",
    "cv2.moveWindow(window_name_combined, width, height)\n",
    "image_blur_median = cv2.medianBlur(image_blur, 3)\n",
    "cv2.imshow(window_name_combined, image_blur_median)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.annotation.grasp_annotation import get_manual_grasp_annotation\n",
    "\n",
    "grasp_pose = get_manual_grasp_annotation(\n",
    "    sample.image_left, sample.depth_map, point_cloud_filtered, X_W_C, intrinsics, log_to_rerun=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloth-competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
