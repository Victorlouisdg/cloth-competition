{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started ğŸš€\n",
    "\n",
    "Welcome to the ICRA 2024 Cloth Competition! In this notebook we will load and explore the data.\n",
    "\n",
    "Run the cell below to download a part of the dataset (10 samples, this is ~1 GB) and unzip it.\n",
    "You only need to run the cell once, then you can comment it out.\n",
    "\n",
    "â˜ï¸ For the full dataset, see: https://cloud.ilabt.imec.be/index.php/s/Sy945rbamg8JMgR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Directory structure ğŸ“‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import fields\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from airo_camera_toolkit.point_clouds.conversions import point_cloud_to_open3d\n",
    "from airo_dataset_tools.data_parsers.pose import Pose\n",
    "from cloth_tools.annotation.grasp_annotation import GraspAnnotation\n",
    "from cloth_tools.dataset.format import load_competition_observation\n",
    "from cloth_tools.dataset.download import download_and_extract_dataset\n",
    "from cloth_tools.visualization.opencv import draw_pose\n",
    "\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "dataset_dir = data_dir / \"cloth_competition_dataset_0000_0-9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we download a small part (10 episodes) of the dataset if no dataset was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji(dir: str, file: str) -> str:\n",
    "    if os.path.isdir(os.path.join(dir, file)):\n",
    "        return \"ğŸ“\"\n",
    "    elif file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "        return \"ğŸ–¼ï¸\"\n",
    "    elif file.endswith(\".mp4\"):\n",
    "        return \"ğŸ¥\"\n",
    "    return \"ğŸ“„\"\n",
    "\n",
    "print(\"First directories in the dataset:\")\n",
    "for f in sorted(os.listdir(dataset_dir))[:5]:\n",
    "    print(emoji(dataset_dir, f), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One sample in the dataset corresponds to one episode. \n",
    "An episode consists of one attempt at unfolding a piece of hanging cloth by grasping it at a human-annotated point.\n",
    "\n",
    "A sample directory contains the following files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = dataset_dir / \"sample_000000\"\n",
    "\n",
    "for f in os.listdir(sample_dir):\n",
    "    print(emoji(sample_dir, f), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One sample thus contains two observations, the **start** and **result**, a **grasp** annotation and a video of the episode.\n",
    "\n",
    "* ğŸ” The **start** observation is taken after the cloth has been grasped by its lowest point.\n",
    "* ğŸ” The **result** observation is taken after the attempt to unfold it.\n",
    "* ğŸ‘‰ The **grasp** pose annotation used to unfold the garment, currently these are human-annotated.\n",
    "* ğŸ¥ The **video** of the entire episode.\n",
    "\n",
    "Participants of the ICRA 2024 Cloth Competition will be asked to predict a good **grasp**, given the **start** observation.\n",
    "\n",
    "The grasp will be evaluated based on the **result** observation (using the surface area of cloth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start Observation ğŸ”\n",
    "\n",
    "In this section we explore some of the data contained in the start observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_start_dir = sample_dir / \"observation_start\"\n",
    "\n",
    "observation = load_competition_observation(observation_start_dir)\n",
    "\n",
    "print(\"Overview of the fields in an Cloth Competition Observation:\")\n",
    "for field in fields(observation):\n",
    "    field_name = field.name + \":\"\n",
    "    field_value = getattr(observation, field.name)\n",
    "    if isinstance(field_value, np.ndarray):\n",
    "        print(f\" - {field_name:<34} np.ndarray {field_value.shape} {field_value.dtype}\")\n",
    "    else:\n",
    "        print(f\" - {field_name:<34} {field.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_dir = sample_dir / \"grasp\"\n",
    "\n",
    "for f in os.listdir(grasp_dir):\n",
    "    print(emoji(grasp_dir, f), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_pose_file = grasp_dir / \"grasp_pose.json\"\n",
    "grasp_annotation_file = grasp_dir / \"grasp_annotation.json\"\n",
    "\n",
    "with open(grasp_pose_file, \"r\") as f:\n",
    "    grasp_pose = Pose.model_validate_json(f.read()).as_homogeneous_matrix()\n",
    "\n",
    "\n",
    "with open(grasp_annotation_file, \"r\") as f:\n",
    "    grasp_annotation = GraspAnnotation.model_validate_json(f.read())\n",
    "\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(\"Grasp pose:\\n\", grasp_pose)\n",
    "    print(\"\\nGrasp annotation:\\n\", grasp_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Result Observation ğŸ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_result_dir = sample_dir / \"observation_result\"\n",
    "\n",
    "observation_result = load_competition_observation(observation_result_dir)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(observation_result.image_left)\n",
    "plt.title(\"Result: image of cloth after grasping and stretching\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â„¹ï¸ The precise calculation of the evaluation metric will be released at a later date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Coordinate frames ğŸ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_W_C = observation.camera_pose_in_world\n",
    "X_W_TCPL = observation.arm_left_tcp_pose_in_world\n",
    "X_W_TCPR = observation.arm_right_tcp_pose_in_world\n",
    "X_W_LB = observation.arm_left_pose_in_world\n",
    "X_W_RB = observation.arm_right_pose_in_world\n",
    "intrinsics = observation.camera_intrinsics\n",
    "\n",
    "X_W_GRASP = grasp_pose\n",
    "\n",
    "image_bgr = cv2.cvtColor(observation.image_left, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "draw_pose(image_bgr, np.identity(4), intrinsics, X_W_C, 0.25)\n",
    "draw_pose(image_bgr, X_W_LB, intrinsics, X_W_C)\n",
    "draw_pose(image_bgr, X_W_RB, intrinsics, X_W_C)\n",
    "draw_pose(image_bgr, X_W_TCPL, intrinsics, X_W_C, 0.05)\n",
    "draw_pose(image_bgr, X_W_TCPR, intrinsics, X_W_C, 0.05)\n",
    "draw_pose(image_bgr, X_W_GRASP, intrinsics, X_W_C, 0.05)\n",
    "\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(image_rgb)\n",
    "plt.title(\"Coordinate frames visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "â” If you have any questions, feel free to ask in on the [Github Discussions page](https://github.com/Victorlouisdg/cloth-competition/discussions)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "from airo_camera_toolkit.point_clouds.conversions import point_cloud_to_open3d\n",
    "from cloth_tools.visualization.opencv import draw_pose\n",
    "\n",
    "# Load the start observation\n",
    "observation_start_dir = sample_dir / \"observation_start\"\n",
    "observation_start = load_competition_observation(observation_start_dir)\n",
    "\n",
    "# Load the result observation\n",
    "observation_result_dir = sample_dir / \"observation_result\"\n",
    "observation_result = load_competition_observation(observation_result_dir)\n",
    "\n",
    "# Get the grasp pose\n",
    "with open(grasp_pose_file, \"r\") as f:\n",
    "    grasp_pose = Pose.model_validate_json(f.read()).as_homogeneous_matrix()\n",
    "\n",
    "# Create the figure with 2 columns and 3 rows\n",
    "fig, axes = plt.subplots(3, 2, figsize=(5, 7), dpi=100, gridspec_kw={'height_ratios': [1, 1, 1]})\n",
    "\n",
    "# --- First column: Start observation ---\n",
    "\n",
    "# RGB image with grasp pose\n",
    "image_bgr = cv2.cvtColor(observation_start.image_left, cv2.COLOR_RGB2BGR)\n",
    "draw_pose(image_bgr, grasp_pose, intrinsics, X_W_C, 0.1)  # Visualize grasp pose\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "from airo_camera_toolkit.image_transforms.transforms.crop import Crop\n",
    "\n",
    "x_middle = observation_start.depth_map.shape[1] // 2\n",
    "width = 352  # mutliple of 32, which is generally preferred for neural networks\n",
    "\n",
    "x = x_middle - width // 2\n",
    "y = 140\n",
    "height = 750 \n",
    "\n",
    "width_result = 1000\n",
    "x_result = x_middle - width_result // 2\n",
    "\n",
    "crop_depth_start = Crop(observation_start.depth_map.shape, x=x, y=y, w=width, h=height)\n",
    "crop_depth_result = Crop(observation_result.depth_map.shape, x=x_result, y=y, w=width_result, h=height)\n",
    "\n",
    "crop_rgb_left_start = Crop(observation_start.image_left.shape, x=x, y=y, w=width, h=height)\n",
    "crop_rgb_left_result = Crop(observation_result.image_left.shape, x=x_result, y=y, w=width_result, h=height)\n",
    "\n",
    "image_start_cropped = crop_rgb_left_start.transform_image(image_rgb)\n",
    "image_result_cropped = crop_rgb_left_result.transform_image(observation_result.image_left)\n",
    "\n",
    "\n",
    "depth_cropped_start = crop_depth_start.transform_image(observation.depth_map)\n",
    "depth_cropped_result = crop_depth_result.transform_image(observation_result.depth_map)\n",
    "\n",
    "\n",
    "distance_max = 1.55\n",
    "\n",
    "segmentation_mask = observation_start.depth_map < distance_max\n",
    "segmentation_mask_cropped = crop_depth_start.transform_image(segmentation_mask)\n",
    "\n",
    "# save cropped RGB start and result images\n",
    "cv2.imwrite(\"start_RGB.png\", cv2.cvtColor(image_start_cropped, cv2.COLOR_RGB2BGR))\n",
    "cv2.imwrite(\"result_RGB.png\", cv2.cvtColor(image_result_cropped, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "# turn off all axes\n",
    "# for ax in axes.flatten():\n",
    "#     ax.axis(\"off\")\n",
    "\n",
    "\n",
    "# axes[0, 0].set_title(\"Start\")\n",
    "# axes[0, 0].imshow(image_start_cropped)\n",
    "# depth_map_im_start = axes[1, 0].imshow(depth_cropped_start, cmap='viridis_r')\n",
    "# depth_map_im_start.set_clim(vmin=1.20, vmax=1.55)  # \n",
    "\n",
    "\n",
    "\n",
    "# axes[2, 0].imshow(depth_cropped_start)\n",
    "\n",
    "# # RGB image without grasp\n",
    "# axes[0, 1].set_title(\"Result\")\n",
    "# axes[0, 1].imshow(image_result_cropped)\n",
    "# axes[1, 1].imshow(depth_cropped_result)\n",
    "# axes[2, 1].imshow(depth_cropped_result)\n",
    "\n",
    "# # Adjust layout and show the figure\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from cloth_tools.visualization.opencv import draw_pose\n",
    "\n",
    "def colorize_depth_and_draw_pose(depth_map, grasp_pose, intrinsics, X_W_C, output_filename, vmin, vmax, crop):\n",
    "    \"\"\"\n",
    "    Colorizes a depth map with 'viridis_r' colormap, draws a pose on it, and saves the result as a PNG image.\n",
    "\n",
    "    Args:\n",
    "        depth_map: The depth map as a NumPy array.\n",
    "        grasp_pose: The grasp pose as a 4x4 homogeneous matrix.\n",
    "        intrinsics: The camera intrinsics matrix.\n",
    "        X_W_C: The camera pose in the world frame as a 4x4 homogeneous matrix.\n",
    "        output_filename: The filename for the output PNG image.\n",
    "    \"\"\"\n",
    "\n",
    "    depth_map = np.clip(depth_map, vmin, vmax)\n",
    "\n",
    "\n",
    "    # Normalize depth map to 0-1 range\n",
    "    normalized_depth = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())\n",
    "\n",
    "    # Apply 'viridis_r' colormap using Matplotlib\n",
    "    cmap = plt.get_cmap('viridis_r')\n",
    "    colored_depth = (cmap(normalized_depth)[:, :, :3] * 255).astype(np.uint8)\n",
    "\n",
    "    # Draw the pose on the colored depth map\n",
    "    if grasp_pose is not None:\n",
    "        # BGR to RGB\n",
    "        colored_depth = cv2.cvtColor(colored_depth, cv2.COLOR_RGB2BGR)\n",
    "        draw_pose(colored_depth, grasp_pose, intrinsics, X_W_C, 0.1)\n",
    "        colored_depth = cv2.cvtColor(colored_depth, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    colored_depth_crop = crop.transform_image(colored_depth)\n",
    "\n",
    "    # Save the image\n",
    "    cv2.imwrite(output_filename, cv2.cvtColor(colored_depth, cv2.COLOR_RGB2BGR))\n",
    "    return colored_depth_crop\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# colorize_depth_and_draw_pose(\n",
    "#     depth_cropped_start, grasp_pose, intrinsics, X_W_C, \"output_depth_with_pose.png\"\n",
    "# )\n",
    "\n",
    "colored_depth_crop = colorize_depth_and_draw_pose(\n",
    "    observation_start.depth_map, grasp_pose, intrinsics, X_W_C, \"output_depth_with_pose.png\", 1.17, 1.54, crop_depth_start\n",
    ")\n",
    "\n",
    "plt.imshow(colored_depth_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colored_depth_crop2 = colorize_depth_and_draw_pose(\n",
    "    observation_result.depth_map, None, intrinsics, X_W_C, \"output_depth_with_pose2.png\", 0.75, 1.20, crop_depth_result\n",
    ")\n",
    "\n",
    "plt.imshow(colored_depth_crop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cropped colored depth start and result images\n",
    "cv2.imwrite(\"start_depth.png\", cv2.cvtColor(colored_depth_crop, cv2.COLOR_RGB2BGR))\n",
    "cv2.imwrite(\"result_depth.png\", cv2.cvtColor(colored_depth_crop2, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun as rr\n",
    "\n",
    "observation_start_dir = dataset_dir / \"sample_000000\" / \"observation_start\"\n",
    "\n",
    "\n",
    "observation = load_competition_observation(observation_start_dir)\n",
    "\n",
    "confidence_map = observation.confidence_map\n",
    "point_cloud = observation.point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloth_tools.point_clouds.operations import filter_point_cloud\n",
    "from cloth_tools.bounding_boxes import BBOX_CLOTH_IN_THE_AIR, bbox_to_mins_and_sizes\n",
    "\n",
    "\n",
    "confidence_threshold = 1.0\n",
    "confidence_mask = (confidence_map <= confidence_threshold).reshape(-1)  # Threshold and flatten\n",
    "# point_cloud_filtered = filter_point_cloud(point_cloud, confidence_mask)\n",
    "\n",
    "# bbox = BBOX_CLOTH_IN_THE_AIR\n",
    "bbox = (-1.0, -1.0, -0.5), (0.5, 1.0, 1.0)\n",
    "\n",
    "\n",
    "point_cloud_cropped = filter_and_crop_point_cloud(point_cloud, confidence_map, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_name = \"start_result\"\n",
    "rr.init(window_name, spawn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_point_cloud = rr.Points3D(positions=point_cloud_cropped.points, colors=point_cloud_cropped.colors)\n",
    "rr.log(\"world/point_cloud\", rr_point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloth-competition-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
